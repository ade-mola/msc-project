{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78749af6f8397604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:35.777443Z",
     "start_time": "2023-10-03T08:50:32.691875Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "\n",
    "from vision_models.alexnet import AlexNet\n",
    "from vision_models.resnet import ResNet, cfgs, resnet50_config\n",
    "from vision_models.vgg import VGG, get_vgg_layers, vgg_configs\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_auc_score, roc_curve, auc, precision_recall_curve, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from matplotlib.colors import Normalize\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import collections\n",
    "import math\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c14f463f84a81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:39.182706Z",
     "start_time": "2023-10-03T08:50:39.164930Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 1996\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db974591f040d81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:40.461633Z",
     "start_time": "2023-10-03T08:50:40.429856Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4992, 0.4839, 0.4827], std=[0.2325, 0.2332, 0.2327]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4992, 0.4839, 0.4827], std=[0.2325, 0.2332, 0.2327]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f84373ec69c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:41.624368Z",
     "start_time": "2023-10-03T08:50:41.583134Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "DATA_DIR = (\n",
    "    \"CarDD_release_folders_single\"\n",
    ")\n",
    "TRAIN_FOLDERS = [\"train\"]\n",
    "VAL_TEST_FOLDERS = [\"val\", \"test\"]\n",
    "\n",
    "try:\n",
    "    # Create train datasets\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        os.path.join(DATA_DIR, TRAIN_FOLDERS[0]), transform=transform_train\n",
    "    )\n",
    "\n",
    "    # Create validation and test datasets\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        os.path.join(DATA_DIR, VAL_TEST_FOLDERS[0]), transform=transform_test\n",
    "    )\n",
    "    test_dataset = datasets.ImageFolder(\n",
    "        os.path.join(DATA_DIR, VAL_TEST_FOLDERS[1]), transform=transform_test\n",
    "    )\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please make sure the data directory is correct.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}. An error occurred while creating the datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499d800564fd723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:42.771830Z",
     "start_time": "2023-10-03T08:50:42.707694Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_iter = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "valid_iter = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size, shuffle=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41c64b86bfa018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:45.608858Z",
     "start_time": "2023-10-03T08:50:45.570868Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_images_per_category(base_dir):\n",
    "    # Initialize a dictionary to store image counts per category\n",
    "    category_counts = {}\n",
    "\n",
    "    # Initialize a variable to keep track of the total image count\n",
    "    total_count = 0\n",
    "\n",
    "    # Iterate through the subdirectories (categories)\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_dir):\n",
    "            # Count the number of image files in the category directory\n",
    "            image_count = len(\n",
    "                [\n",
    "                    file\n",
    "                    for file in os.listdir(category_dir)\n",
    "                    if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\"))\n",
    "                ]\n",
    "            )\n",
    "            category_counts[category] = image_count\n",
    "\n",
    "            # Add the count to the total\n",
    "            total_count += image_count\n",
    "\n",
    "    # Include the total count in the dictionary\n",
    "    category_counts[\"Total\"] = total_count\n",
    "\n",
    "    return category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0bd6dc95472a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:48.999729Z",
     "start_time": "2023-10-03T08:50:48.947702Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_images_per_category(\n",
    "    \"CarDD_release_folders_single/train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d3a85300750ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:50.152490Z",
     "start_time": "2023-10-03T08:50:50.114910Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_images_per_category(\n",
    "    \"CarDD_release_folders_single/val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a89caa2300902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:52.773785Z",
     "start_time": "2023-10-03T08:50:52.757590Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# means = torch.zeros(3)\n",
    "# stds = torch.zeros(3)\n",
    "\n",
    "# for img, label in train_dataset:\n",
    "#     means += torch.mean(img, dim = (1,2))\n",
    "#     stds += torch.std(img, dim = (1,2))\n",
    "\n",
    "# means /= len(train_dataset)\n",
    "# stds /= len(train_dataset)\n",
    "\n",
    "# print(f'Calculated means: {means}')\n",
    "# print(f'Calculated stds: {stds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6283dd6459c5ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.061602Z",
     "start_time": "2023-10-03T08:50:53.450567Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min=image_min, max=image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_images(images, labels, classes, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot a grid of images with their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        images (list of tensors): A list of images to plot.\n",
    "        labels (list of int): A list of corresponding labels for each image.\n",
    "        classes (list of str): A list of class names.\n",
    "        normalize (bool): Whether to normalize the images.\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    rows = cols = int(n_images ** 0.5)\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        image = images[i]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        label = classes[labels[i]]\n",
    "        ax.set_title(label)\n",
    "        ax.axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "N_IMAGES = 10\n",
    "images, labels = next(iter(train_iter))\n",
    "image_label_pairs = list(zip(images, labels))\n",
    "random.shuffle(image_label_pairs)\n",
    "shuffled_images, shuffled_labels = zip(*image_label_pairs)\n",
    "classes = train_dataset.classes\n",
    "\n",
    "fig = plot_images(shuffled_images[:N_IMAGES], shuffled_labels[:N_IMAGES], classes)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d639d10fcffe935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.557618Z",
     "start_time": "2023-10-03T08:50:58.096331Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b2ba0d62f0c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.558311Z",
     "start_time": "2023-10-03T08:50:58.103682Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d6d2c6af5f2c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.558574Z",
     "start_time": "2023-10-03T08:50:58.111410Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {parameters:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f8eeb8e51d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.559182Z",
     "start_time": "2023-10-03T08:50:58.139893Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_topk_accuracy(y_pred, y, k=2):\n",
    "    with torch.no_grad():\n",
    "        batch_size = y.shape[0]\n",
    "        _, top_pred = y_pred.topk(k, 1)\n",
    "        top_pred = top_pred.t()\n",
    "        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n",
    "        correct_1 = correct[:1].reshape(-1).float().sum(0, keepdim=True)\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        acc_1 = correct_1 / batch_size\n",
    "        acc_k = correct_k / batch_size\n",
    "    return acc_1, acc_k\n",
    "\n",
    "\n",
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119aa3ae946c1b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.559459Z",
     "start_time": "2023-10-03T08:50:58.146194Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, scheduler, device):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data iterator.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to train.\n",
    "        iterator: A PyTorch data iterator that generates (x, y) tuples.\n",
    "        optimizer: The PyTorch optimizer used for training.\n",
    "        criterion: A PyTorch loss function to compute the loss.\n",
    "        scheduler: The learning rate scheduler used for training.\n",
    "            If no learning rate scheduler is used, set this to None.\n",
    "        device: The device to use for evaluation (e.g., \"cpu\" or \"cuda\" or \"mps\").\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the epoch loss, top-1 accuracy, and top-5 accuracy\n",
    "        (if applicable).\n",
    "    \"\"\"\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_1 = 0\n",
    "    epoch_acc_5 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if isinstance(model, ResNet):\n",
    "            # Calculate top-1 and top-5 accuracy for ResNet models\n",
    "            y_pred, _ = model(x)\n",
    "            acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
    "            epoch_acc_1 += acc_1.item()\n",
    "            epoch_acc_5 += acc_5.item()\n",
    "        else:\n",
    "            # Calculate regular accuracy for other models\n",
    "            y_pred = model(x)\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "            epoch_acc_1 += acc.item()\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc_1 /= len(iterator)\n",
    "\n",
    "    return epoch_loss, epoch_acc_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55f26d15e4f2d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.559603Z",
     "start_time": "2023-10-03T08:50:58.154159Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given data iterator.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to evaluate.\n",
    "        iterator: A PyTorch data iterator that generates (x, y) tuples.\n",
    "        criterion: A PyTorch loss function to compute the loss.\n",
    "        device: The device to use for evaluation (e.g., \"cpu\" or \"cuda\" or \"mps\").\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the epoch loss, top-1 accuracy, and top-5 accuracy\n",
    "        (if applicable).\n",
    "    \"\"\"\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_1 = 0\n",
    "    epoch_acc_5 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            if isinstance(model, ResNet):\n",
    "                # Calculate top-1 and top-5 accuracy for ResNet models\n",
    "                y_pred, _ = model(x)\n",
    "                acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
    "                epoch_acc_1 += acc_1.item()\n",
    "                epoch_acc_5 += acc_5.item()\n",
    "            else:\n",
    "                # Calculate regular accuracy for other models\n",
    "                y_pred = model(x)\n",
    "                acc = calculate_accuracy(y_pred, y)\n",
    "                epoch_acc_1 += acc.item()\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc_1 /= len(iterator)\n",
    "\n",
    "    return epoch_loss, epoch_acc_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a7854b1f5cbf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.560804Z",
     "start_time": "2023-10-03T08:50:58.162707Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, iterator):\n",
    "    \"\"\"\n",
    "    Get predictions for a PyTorch model on a given data iterator.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to get predictions for.\n",
    "        iterator: The data iterator for getting predictions.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the predicted images, labels, and probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in tqdm(iterator):\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            if isinstance(model, ResNet):\n",
    "                y_pred, _ = model(x)\n",
    "            else:\n",
    "                y_pred = model(x)\n",
    "\n",
    "            y_prob = F.softmax(y_pred, dim=-1)\n",
    "\n",
    "            images.append(x.cpu())\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "    images = torch.cat(images, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    probs = torch.cat(probs, dim=0)\n",
    "\n",
    "    return images, labels, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6741c4d278565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:58.964922Z",
     "start_time": "2023-10-03T08:50:58.701243Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d802a001c32813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:50:59.037331Z",
     "start_time": "2023-10-03T08:50:58.957576Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_eval(EPOCHS, model, optimizer, scheduler):\n",
    "\n",
    "    model = model.to(device)\n",
    "    counter = 0\n",
    "    patience = 8\n",
    "    best_valid_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_val_acc = float('inf')\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in trange(EPOCHS, desc=\"EPOCHS\"):\n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iter, optimizer, criterion, scheduler, device)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iter, criterion, device)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_val_acc = valid_acc\n",
    "            best_epoch = epoch\n",
    "            filename = f\"{model.__class__.__name__}_{optimizer.__class__.__name__}.pt\"\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            # increase patience counter on no improvement\n",
    "            if counter >= patience:\n",
    "                print(f\"Validation loss hasn't improved in {patience} epochs. Stopping early.\")\n",
    "                break\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        train_accuracy.append(train_acc)\n",
    "        valid_accuracy.append(valid_acc)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% |\")\n",
    "        print(f\"\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:6.2f}% |\")\n",
    "\n",
    "    print(f\"Best epoch: {best_epoch+1}, Best validation accuracy: {best_val_acc}, Best validation loss: {best_valid_loss}\")\n",
    "\n",
    "    return train_accuracy, valid_accuracy, train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb4b7c2cb7f06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:51:00.001775Z",
     "start_time": "2023-10-03T08:50:59.954201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('CarDD_release_folders_single/ground_truth.csv')\n",
    "y_test = np.array(test.drop(['Title'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e438dc4aae4423b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:51:00.652325Z",
     "start_time": "2023-10-03T08:51:00.622757Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_test, probs, class_labels, NN):\n",
    "    \"\"\"\n",
    "    Plots the ROC curves for a given set of true labels and predicted probabilities for each class.\n",
    "    \n",
    "    Args:\n",
    "    y_test (np.array): True class labels of shape (n_samples, n_classes).\n",
    "    probs (np.array): Predicted probabilities of shape (n_samples, n_classes).\n",
    "    class_labels (list): List of class labels.\n",
    "    \"\"\"\n",
    "    n_classes = len(class_labels)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "\n",
    "    colors = ['darkorange', 'green', 'blue', 'red', 'purple', 'yellow']\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,\n",
    "                 label=f'ROC curve of {class_labels[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot macro-average ROC curve\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', lw=2, linestyle=':',\n",
    "             label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})')\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', lw=2, linestyle=':',\n",
    "             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{NN} ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f\"{NN}_roc_curve.png\", bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a570bbd5b4e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:51:01.394310Z",
     "start_time": "2023-10-03T08:51:01.385869Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Plot confusion matrix using matplotlib and sklearn metrics.\n",
    "\n",
    "    Args:\n",
    "    - y_true (np.ndarray): true labels\n",
    "    - y_pred (np.ndarray): predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_names = unique_labels(y_true, y_pred)\n",
    "\n",
    "    # Plot raw confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, y_pred, normalize=\"true\", cmap=plt.cm.Blues)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    tp = cm[0, 0]\n",
    "    fp = cm[1, 0]\n",
    "    fn = cm[0, 1]\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=['dent', 'glass shatter',\n",
    "                                                               'lamp broken', 'scratch', 'tire flat']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138be049074797a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:51:02.240818Z",
     "start_time": "2023-10-03T08:51:02.225458Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_loss_acc(NN, train_losses, valid_losses, train_acc, valid_acc):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    fig.suptitle(f\"Loss and Accuracy plots for {NN}\")\n",
    "\n",
    "    axs[0].plot(train_losses, label='Training loss')\n",
    "    axs[0].plot(valid_losses, label='Validation loss')\n",
    "    axs[0].set_xlabel(\"Epochs\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].legend(frameon=False)\n",
    "\n",
    "    axs[1].plot(train_acc, label='Training Accuracy')\n",
    "    axs[1].plot(valid_acc, label='Validation Accuracy')\n",
    "    axs[1].set_xlabel(\"Epochs\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].legend(frameon=False)\n",
    "\n",
    "    plt.savefig(f\"{NN}_plot.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4367ea",
   "metadata": {},
   "source": [
    "### Download pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4a5ed2d596b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:51:07.666646Z",
     "start_time": "2023-10-03T08:51:03.185809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg11_bn = models.vgg11_bn(weights='DEFAULT')\n",
    "vgg16_bn = models.vgg16_bn(weights='DEFAULT')\n",
    "vgg19_bn = models.vgg19_bn(weights='DEFAULT')\n",
    "\n",
    "alexnet = models.alexnet(weights='DEFAULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e47b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate in_features into VGG classifier and AdaptivePool output_size \n",
    "\n",
    "def calculate_out_features_and_adaptivepool(feature_extractor, input_size):\n",
    "    in_features = torch.randn(1, 3, input_size, input_size)\n",
    "    out_features = feature_extractor(in_features).view(1, -1).size(1)\n",
    "    \n",
    "    hxw = int(np.sqrt((out_features / 2) / 256))\n",
    "\n",
    "    # for alexnet\n",
    "    # hxw = int(np.sqrt(out_features / 256))\n",
    "    \n",
    "    return out_features, hxw\n",
    "\n",
    "\n",
    "feature_extractor = models.vgg11(pretrained=True).features\n",
    "input_size = 128 \n",
    "\n",
    "out_features, hxw = calculate_out_features_and_adaptivepool(feature_extractor, input_size)\n",
    "\n",
    "print(\"out_features:\", out_features)\n",
    "print(\"hxw:\", hxw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, model_name, output_size, in_features, architecture='VGG', num_classes=5, lr=1e-4, l2=False, epochs=10, device=device, optimizer='Adamax'):\n",
    "    \"\"\"\n",
    "    A function to run the entire pipeline: model modification, training, evaluation, and plotting.\n",
    "    \"\"\"\n",
    "    # Modify the model\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size=output_size)\n",
    "    \n",
    "    if architecture == 'VGG':\n",
    "        new_classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    elif architecture == 'AlexNet':\n",
    "        new_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported architecture: {architecture}\")\n",
    "\n",
    "    model.classifier[0].in_features = in_features\n",
    "    model.classifier = new_classifier\n",
    "    \n",
    "    count_parameters(model)\n",
    "    \n",
    "    # Prepare parameters and optimizers\n",
    "    params = [\n",
    "        {'params': model.features.parameters(), 'lr': lr / 10},\n",
    "        {'params': model.classifier.parameters()}\n",
    "    ]\n",
    "\n",
    "    if l2:\n",
    "        optimizers = {\n",
    "            'AdamW': optim.AdamW(params, lr=lr, weight_decay=0.01),\n",
    "            'Adamax': optim.Adamax(params, lr=lr, weight_decay=0.01),\n",
    "            'SGD': optim.SGD(params, lr=lr, weight_decay=0.01)\n",
    "        }\n",
    "    else:\n",
    "        optimizers = {\n",
    "            'AdamW': optim.AdamW(params, lr=lr),\n",
    "            'Adamax': optim.Adamax(params, lr=lr),\n",
    "            'SGD': optim.SGD(params, lr=lr)\n",
    "        }\n",
    "    \n",
    "    chosen_optimizer = optimizers.get(optimizer)\n",
    "    if chosen_optimizer is None:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "        \n",
    "    # Train and evaluate the model\n",
    "    train_accuracy, valid_accuracy, train_losses, valid_losses = train_eval(epochs, model, chosen_optimizer, None)\n",
    "    \n",
    "    # Plotting and evaluation\n",
    "    plot_loss_acc(model_name, train_losses, valid_losses, train_accuracy, valid_accuracy)\n",
    "    model.to(device)\n",
    "    images, labels, probs = get_predictions(model, test_iter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    plot_roc_curve(y_test, probs, val_dataset.classes, model_name)\n",
    "    plot_confusion_matrix(y_test, probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51378203",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ce5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(alexnet, 'AlexNet', output_size=(3, 3), in_features=2304, architecture='AlexNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bf3ce",
   "metadata": {},
   "source": [
    "### VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b00874",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(vgg11_bn, 'VGG11_bn', output_size=(4, 4), in_features=8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24f2d3",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(vgg16_bn, 'VGG16_bn', output_size=(4, 4), in_features=8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57318787",
   "metadata": {},
   "source": [
    "### With L2 Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(alexnet, 'AlexNet', output_size=(3, 3), in_features=2304, architecture='AlexNet', l2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ab051",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(vgg11_bn, 'VGG11_bn', output_size=(4, 4), in_features=8192, l2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6483e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(vgg16_bn, 'VGG16_bn', output_size=(4, 4), in_features=8192, l2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2dc3b35ce1f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T14:41:13.572448Z",
     "start_time": "2023-09-27T14:41:13.474408Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Predict Images\n",
    "\n",
    "# predicted_labels = torch.argmax(probs, dim=1)\n",
    "\n",
    "# images = images.cpu()\n",
    "# labels = labels.cpu()\n",
    "# random_indices = random.sample(range(len(images)), 10)\n",
    "# # random_indices = list(range(len(images)))[:10]\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(12, 6))\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# mean, std = [0.4992, 0.4839, 0.4827], [0.2260, 0.2268, 0.2264]\n",
    "\n",
    "# for i, idx in enumerate(random_indices):\n",
    "#     image = images[idx].cpu()  # Move image tensor to CPU\n",
    "\n",
    "#     # Apply normalization\n",
    "#     for channel in range(3):\n",
    "#         image[channel] = (image[channel] * std[channel]) + mean[channel]\n",
    "\n",
    "#     image = to_pil_image(image)  # Convert tensor to PIL format\n",
    "#     axes[i].imshow(image)\n",
    "#     axes[i].set_title(f\"Predicted: {val_dataset.classes[predicted_labels[idx]]}\\nTrue: {val_dataset.classes[labels[idx]]}\")\n",
    "#     axes[i].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
